“Do your work, then step back.
The only path to serenity.”

I began this project as an unemployed but hungry person who was trying to teach himself how to program by developing an app that said person thought was interesting, challenging, and useful.

I have a job now, and because of that, my brain is preoccupied with work-related matters. Furthermore, I have realized that while I have often concerned myself with how my thinking affects my work and creativity, I have not considered how my work affects my thinking and my being. I believe the habits of my mind have become too rigid to reveal to me their own limitations. This is all to say, I have to step back.

The name of my app is Locationary. It’s a point-of-interest finder that shows you all the mountain peaks, trails, lakes, rivers, and streams by pointing your camera in the direction of the land features you’re interested in. The easiest way to describe it is that it’s like SkyView for the earth. Am I allowed to say that, or is SkyView trademarked?

What follows is a broad, technical overview. When you open the app, it determines your location and makes a query to the Geonames database for relevant points of interest within 100 miles of where you are. The NearbyPointsViewController is the delegate for the locationManager, the GeonamesCommunicator retrieves the data and the GeoNamesJSONParser parses them. For each point returned by the Geonames service, I do a rough elevation profile calculation using the Geospatial Database Abstraction Layer library to determine if you can actually see the point or if there’s another mountain or other feature that’s obstructing your view of the point. All of this is done in ElevationDataManager, which has to do several handoffs (ElevationDataManager -> GDALWrapper -> ObjC+GDAL -> gdalLocation.cpp) to get Swift to talk to the actual GDAL library, which is in C++. If you can see the point, it gets added to the nearbyPointsInLineOfSight array in NearbyPointsManager, which keeps track of *every* *visible* nearbyPoint. If you can not see the point, it goes back to the shadow from which it came. All of the supporting GDAL files can be found in the goal-include folder, and the elevation data itself in the us_bounded_compressed_tiled.tif file at the project’s root. The app currently only works in the United States because this file is 25 mb large. The global equivalent would probably create a file that’s well over 200 mb. There are also probably many people who know a lot more about this stuff than I do.

I use the phone’s accelerometer motion handler, where I am also grabbing the phone’s heading with each accelerometer update, to calculate the virtual point’s position on the screen. Thus, if a physical point, say a mountain peak, were at the same elevation as you a mile away (i.e. a distance too small for the curvature of the earth to have an appreciable effect on relative elevation of the point), and the mountain peak were directly in the center of the phone’s screen in the left-right direction, the virtual point that I display on the phone will appear directly in the middle of the phone. Tapping on the point will reveal the point’s name, its coordinates, its elevation, and its distance from your current location. All of this is done in the iOS “god class,” the NearbyPointsViewController.

Currently, you must tap on the screen to actually see the points with the field of vision of the phone’s camera. It functions this way because there are scenarios—say at the top of a fire tower on top of a mountain—where you will see a *lot* of points. The phone simply cannot handle updating 50+ views on the screen 60 times a second. I have added an animation that makes the virtual points appear to be weighted, so that as you move the phone left and right, the top of the virtual point will move before the bottom, like a fishing lure in water. This certainly makes it harder for the phone to update the points on the screen as the phone gets moved, turned, and swiveled, but I believe this animation is important to make the app feel friendly and lively. Friends are important, and being lively is an important feature of being a good friend.

Lastly, because of the performance issues of updating a lot of virtual points simultaneously, the app will only show 25 points at a time. If there are more than 25 points within the camera’s field of vision, you can vertical pan up and down to view the smaller or taller mountains. If you double-tap on the screen, a vertical pan will then show the mountains by distance from your current location. Continue to double-tap to toggle between the two ways to ‘sort’ the points.

I would say about 60% of the code base is unit tested, and I want to get the rest of it covered, all of which will take time.

For the next step, I’d like each virtual point to show a small image of its elevation profile from the point of view of the user’s current location. This will help the user identify the peak among of a sea of peaks that the user will be looking at on their phone. I don’t know if this is possible or even feasible—with what kind of resolution can we build the image? I am open to suggestions, ideas, thoughts, criticisms, concerns, questions, and feelings. Feelings are very important.

There have been many times when I thought I could not get this to work. Getting the elevation profile using GDAL was a real *$#@%! I can say with complete confidence that this app can be something great. Even if we can not get this to work as we want, there is great value in our trying. Because in trying, we will learn something new, possibly better, and hopefully beautiful. That inspires me.

“Darkness within darkness.
The gateway to all understanding.”

There are many persons for whom I am so grateful for so much support and patience, and one in particular. Thank you so much!
